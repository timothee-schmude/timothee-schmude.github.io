<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Timoth√©e Schmude </title> <meta name="author" content="Timoth√©e Schmude"> <meta name="description" content="Website of Timoth√©e Schmude. "> <meta name="keywords" content="timothee-schmude, artificial intelligence, explainable ai, contestable ai, university of vienna, ai novices"> <meta property="og:site_name" content="Timoth√©e Schmude"> <meta property="og:type" content="website"> <meta property="og:title" content="Timoth√©e Schmude | about"> <meta property="og:url" content="https://timothee-schmude.github.io//"> <meta property="og:description" content="Website of Timoth√©e Schmude. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="Website of Timoth√©e Schmude. "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Timoth√©e Schmude"
        },
        "url": "https://timothee-schmude.github.io//",
        "@type": "WebSite",
        "description": "Website of Timoth√©e Schmude.
",
        "headline": "about",
        
        "name": "Timoth√©e Schmude",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://timothee-schmude.github.io//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Timoth√©e</span> Schmude </h1> <p class="desc"><em>I was startled to see how quickly and how very deeply people conversing With DOCTOR became emotionally involved with the computer and how unequivocally they anthropomorphized it.</em> - Joseph Weizenbaum in <a href="https://en.wikipedia.org/wiki/Computer_Power_and_Human_Reason" rel="external nofollow noopener" target="_blank">Computer Power and Human Reason</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/timothee_schmude-480.webp 480w,/assets/img/timothee_schmude-800.webp 800w,/assets/img/timothee_schmude-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/timothee_schmude.jpg?7fc9d215f70a69775cae5348c18f34af" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="timothee_schmude.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hi! I‚Äôm Timoth√©e, a PhD student at the <a href="https://vda.cs.univie.ac.at/team/person/1001666/#publications" rel="external nofollow noopener" target="_blank">University of Vienna</a> working on explainable and contestable AI. My research focuses on how explanations of algorithmic systems are understood by people and uses qualitative methods to evaluate explanation approaches with stakeholders in interview settings. The latest projects examine the information needs and understanding processes of AI novices, explanation designs for group deliberations, and regulation of explainable and contestable AI.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 14, 2024</th> <td> We published a <a href="https://www.sciencedirect.com/science/article/pii/S1071581924001630" rel="external nofollow noopener" target="_blank">paper in the International Journal of Human-Computer Studies</a> on information needs of AI Novices üéâ </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 01, 2024</th> <td> I‚Äôm visiting T√©l√©com Paris in Massy until January 2025 to research policy of explainable and contestable AI </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 18, 2023</th> <td> I was invited to give a keynote talk at the <a href="https://justizministerium.hessen.de/presse/workshop-reihe-zum-thema-kuenstliche-intelligenz-fuer-die-justiz" rel="external nofollow noopener" target="_blank">AI in the Judiciary</a> workshop of the Hessian Ministry of Justice </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="schmude2024_deliberative_xai" class="col-sm-8"> <div class="title">Deliberative XAI: How Explanations Impact Understanding and Decision-Making of AI Novices in Collective and Individual Settings</div> <div class="author"> <em>Timoth√©e Schmude</em>,¬†Laura Koesten,¬†Torsten M√∂ller,¬†and¬†Sebastian Tschiatschek </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>XAI research often focuses on settings where people learn about and assess algorithmic systems individually. However, as more public AI systems are deployed, it becomes essential for XAI to facilitate collective understanding and deliberation. We conducted a task-based interview study involving 8 focus groups and 12 individual interviews to explore how explanations can support AI novices in understanding and forming opinions about AI systems. Participants received a collection of explanations organized into four information categories to solve tasks and decide about a system‚Äôs deployment. These explanations improved or calibrated participants‚Äô self-reported understanding and decision confidence and facilitated group discussions. Participants valued both technical and contextual information and the self-directed and modular explanation structure. Our contributions include an explanation approach that facilitates both individual and collaborative interaction and explanation design recommendations, including active and controllable exploration, different levels of information detail and breadth, and adaptations to the needs of decision subjects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="schmude2024_ai_novices" class="col-sm-8"> <div class="title">Information that matters: Exploring information needs of people affected by algorithmic decisions</div> <div class="author"> <em>Timoth√©e Schmude</em>,¬†Laura Koesten,¬†Torsten M√∂ller,¬†and¬†Sebastian Tschiatschek </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.ijhcs.2024.103380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system‚Äôs decisions, such as domain experts and decision subjects. To address this, we present the ‚ÄúXAI Novice Question Bank‚Äù, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants‚Äô confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants‚Äô prior perceptions of the systems‚Äô risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system‚Äôs deployment, while those who perceived low risks rather asked about the system‚Äôs operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">FAccT ‚Äô23</abbr> </div> <div id="schmude2023" class="col-sm-8"> <div class="title">On the Impact of Explanations on Understanding of Algorithmic Decision-Making</div> <div class="author"> <em>Timoth√©e Schmude</em>,¬†Laura Koesten,¬†Torsten M√∂ller,¬†and¬†Sebastian Tschiatschek </div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>, Chicago, IL, USA, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3593013.3594054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al_On%20the%20Impact%20of%20Explanations%20on%20Understanding%20of%20Algorithmic%20Decision-Making.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by "high-risk" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems‚Äô adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a "high-risk" ADM system to participants and analyse their responses both inductively and deductively, using the "six facets of understanding" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the "six facets" framework is a promising approach to analyse participants‚Äô thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the "dialogue" modality as a valid explanation approach to increase participant engagement and interaction with the "explainer", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants‚Äô perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the "six facets" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%74%69%6D%6F%74%68%65%65.%73%63%68%6D%75%64%65@%75%6E%69%76%69%65.%61%63.%61%74" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/timoth%C3%A9e-schmude-17a032218" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.researchgate.net/profile/Timothee-Schmude/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=clG_VOgAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://stackoverflow.com/users/13440007" title="Stackoverflow" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-stack-overflow"></i></a> </div> <div class="contact-note">Don‚Äôt hesitate to get in touch! I'll most quickly answer via mail </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Timoth√©e Schmude. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>