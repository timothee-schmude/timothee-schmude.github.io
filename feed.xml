<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://timothee-schmude.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://timothee-schmude.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-02T16:18:14+00:00</updated><id>https://timothee-schmude.github.io//feed.xml</id><title type="html">blank</title><subtitle>Website of Timothée Schmude. </subtitle><entry><title type="html">Surprising and Irritating: How to get what people say using abductive analysis (in XAI)</title><link href="https://timothee-schmude.github.io//blog/2023/abduction/" rel="alternate" type="text/html" title="Surprising and Irritating: How to get what people say using abductive analysis (in XAI)"/><published>2023-11-30T10:00:00+00:00</published><updated>2023-11-30T10:00:00+00:00</updated><id>https://timothee-schmude.github.io//blog/2023/abduction</id><content type="html" xml:base="https://timothee-schmude.github.io//blog/2023/abduction/"><![CDATA[<p><em>This is a piece on qualitative methods and abductive analysis in explainable AI. Here’s the TL;DR: Talking to people is a good idea if you want to find out which explanations they need. But analysing what they tell you is a different story, especially if it’s surprising and confusing. To get the most out of it, abductive analysis has three recommendations: revisit your data, defamiliarize yourself, and use alternative theoretical casings. The list at the end explains what these mean.</em></p> <h3 id="discovering-people">Discovering people</h3> <p>Explainable AI is in the process of discovering that what you explain will not necessarily arrive at your audience the way you intended. Instead, your message will be jumbled and warped by all things separating you from them: the computer, the language, the difference in knowledge between you, the way in which you typically explain things to yourself, and so on. Graduating in Media Studies has its payoff at this (and only this) exact point – hear now a sacral choir chanting: “The medium is the message” (<a href="https://en.wikipedia.org/wiki/The_medium_is_the_message">context</a>).</p> <p>Yes, but how does this help in explaining AI? At first, it only suggests to us to take a look beyond the idea of quantifying understanding and perceptions and get a good grasp of the human behind these metrics. In short: Speak to who you’re talking to. Doing so will yield quite surprising insights like ‘people tend to judge a system based on the institution deploying it’ [1] and ‘understanding has both rational and emotional facets’ [2]. These insights can then, with a bit of polishing and interpretation, be fed back into XAI to improve explanation design – at least that’s the idea.</p> <h3 id="talking-to-people">Talking to people</h3> <p>Talking to people is a craft well established in the Social Sciences, which provide a plethora of methods, measures, and concepts to find out how people interact, acquire knowledge, and split the burden of understanding. After XAI discovered humans (I’m exaggerating), several researchers wrote papers that made an effort to build bridges to this fountain of wisdom that is the Social Sciences. Two of the rather important ones were Miller’s summary of things to know about human explanations [3] and Langer et al.’s model of people who have stakes in AI systems [4] (which, to be fair, is based on previous work by Arrieta et al. 2019 [5]). Finally, with a little shove from HCI – a field that discovered humans a good while back – a new sub-field emerged: Human-centred explainable AI, or HCXAI.</p> <p>This is our ‘You are here’ red dot starting point: Explanations are not created in a vacuum nor do they arrive in one, so we would do good to consider all steps of the way. There is an argument to be made that XAI cannot possibly be responsible for how people process the information that we convey, and there might be ‘handoff’-point at which this might be true, but, personally, I feel we have not yet arrived there. Rather, we should first try to talk to all the stakeholders we identified two years back and find out what they would like to know.</p> <h3 id="understanding-what-people-say">Understanding what people say</h3> <p>This brings us to the practical, tutorial-like part of this text. I can now reveal that all of the above was pretence for an excuse to write about the obscure strand of qualitative research called ‘abductive analysis’ – and here we are! Do not quit yet! I’ll promise I’ll try to make it entertaining.</p> <p>Imagine you talked to 30 people that you sourced from the local job agency in a very hot Viennese summer, something I’m incidentally most familiar with. You presented explanations about the system you are trying to study, recorded what people said in response and ended up with 30 one-hour interview transcripts. A giant amount of text for computer science standards, but a dwarf amount compared to what social scientists typically have to battle. The question that abductive analysis – and any qualitative analysis for that matter – tries to answer is: How do you make sense of all the things said?</p> <p>There are two main and not too complex answers to this question: Either you dive into the data and build themes from what people said, or you tap into the literature to find your themes and then see if they connect to what people said. The first one is called inductive, the second one deductive analysis (and yes, I will draw the anger of the qualitative research gods with this simplified account).</p> <p>While there exist strong opinions that either inductive or deductive analysis is the ground truth, a most intuitive way of going about analysing data like these interview transcripts is to combine the two. On the one hand, the literature certainly provides some topics that people typically talk about when trying to understand explanations of AI (such as fairness, the application context, and the technical details), on the other, you might well find things that the literature does not yet cover (such as the developer’s intention and the cost of deploying a model). So as not to spoil your naïve view, you first go through the transcripts and create categories from what you find in the data, and then you look into literature to see if anything matches. Indeed, some things were known before, others do neatly complement what’s already known, but then again, there is data that makes no sense at all. Why does participant 7-A not trust the study procedure? Why does participant 11-B talk about his relationship to his brother when asked about his understanding? And what do you do with the endless opinions that crop up all over your data (‘This whole algorithm thing is nonsense.’)?</p> <h3 id="enter-abductive-analysis">Enter abductive analysis</h3> <p>Abductive analysis makes these idiosyncratic, inconvenient, confusing parts of the data a centrepiece of analysis. After all, why is it not apparent what these parts mean? Because their relation to the study goals is not clear, meaning they relate to something you did not consider, but which apparently does play a role in the real world and therefore – and this is the catch – will also play a role when your or anyone’s AI explanation will be used in the real world to provide information to this audience. This is an actual, not to be underestimated limitation.</p> <p>There are three key parts to abductive analysis that all try to help you in producing interesting data and getting a hold of it to gain insight: revisiting, defamiliarization, and alternative casing. The whole idea goes back to Charles Peirce’s work from the 1930’s and is often summarized with the following triplet:</p> <ul> <li> <p>The surprising fact C is observed.</p> </li> <li> <p>But if A were true, C would be a matter of course.</p> </li> <li> <p>Hence, there is a reason to suspect that A is true. [6]</p> </li> </ul> <p>You will be glad to hear there’s more up-to-date literature describing the method, namely Timmermans’ and Tavory’s paper from 2012 [7] and their book from 2014 [8]. More or less helpfully, they explain:</p> <p>“<em>[A]bduction is the form of reasoning through which we perceive the phenomenon as related to other observations either in the sense that there is a cause and effect hidden from view, in the sense that the phenomenon is seen as similar to other phenomena already experienced and explained in other situations, or in the sense of creating new general descriptions.</em>” [7]</p> <p>In an effort to ground the whole thing, they add that everyone performs abduction whenever they experience something surprising in their life, drawing from their existent knowledge to incorporate the new observations. Two aspects helped me to get into this train of thought: First, ‘abducting’ means leading away: surprising insights lead us away from known theories to new ones. Second, the position of the researcher is important: depending on what you know and how you think, you will produce other insights than the next person, even if you’re from the same field. I put together a diagram for myself that semi-aptly tries to capture the idea. My idea of how abductive analysis arrives at theory: Explanations for surprising evidence are considered and applied by revisiting and defamiliarizing data. Finally, when no theory sufficiently explains a piece of data, a new theory is created.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/abduction_process" sizes="95vw"/> <img src="/assets/img/posts_images/abduction_process" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> My idea of how abductive analysis arrives at theory: Explanations for surprising evidence are considered and applied by revisiting and defamiliarizing data. Finally, when no theory sufficiently explains a piece of data, a new theory is created. </div> <h3 id="the-how-to-of-abductive-analysis">The how-to of abductive analysis</h3> <p>In the spirit of a true crash course I will close this method piece with an attempt to give you something practical to try out (but please do read the paper before trying this at home). Here is my interpretation of how to apply the trinity of abductive analysis in your qualitative work:</p> <ul> <li><strong>Revisit the phenomenon</strong>: Qualitative data is overwhelming, our perception can’t handle all of it in one go. Take time, let the data lie for a bit and come back with fresh eyes. Check your field notes, write memos, try to re-experience your data over a stretch of time and you will find that there’s more to see every time you come back</li> <li><strong>Defamiliarize yourself</strong>: What you know guides what you perceive. Try to alienate yourself from your data, estrange the familiar, put your textual data in spoken word and vice versa to notice things you previously took for granted. Where revisiting is about perceiving data at different times, defamiliarization is about perceiving it from a semantic distance</li> <li><strong>Use alternative casings</strong>: Theories serve as guidelines for how we analyse data, changing theories means changing guidelines. Connect your data to multiple theoretical frameworks to understand the data in as many ways as possible. When analysing data is a puzzle, changing our theoretical positionality will present us with a new puzzle to solve each time. [7, 8]</li> </ul> <p>And with that I leave you. My feelings towards abductive analysis are of fascination and confusion in equal parts and indeed I have turned and twisted these ideas in my head for a few weeks now, trying to find a perspective that allows for easy application. Despite the title of this article, what perspectives I have found so far don’t take the form of a manual, but rather lead to a change in consciousness for how empirical studies allow us to gather and understand data. In my case, it sparked confidence that producing surprising and irritating data is not only allowed, but even desirable.</p> <p>**Inspiring or puzzling? Do let me know what you think about abduction! **</p> <p>Here are the references used:</p> <p>[1] Anna Brown, Alexandra Chouldechova, Emily Putnam-Hornstein, Andrew Tobin, and Rhema Vaithianathan. 2019. Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-making in Child Welfare Services. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, Glasgow Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300271</p> <p>[2] Timothée Schmude, Laura Koesten, Torsten Möller, and Sebastian Tschiatschek. 2023. On the Impact of Explanations on Understanding of Algorithmic Decision-Making. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ’23). Association for Computing Machinery, New York, NY, USA, 959–970. https://doi.org/10.1145/3593013.3594054</p> <p>[3] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (Feb. 2019), 1–38. https://doi.org/10.1016/j.artint.2018.07.007</p> <p>[4] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want from Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research. Artificial Intelligence 296 (July 2021), 103473. https://doi.org/10.1016/j.artint.2021.103473</p> <p>[5] A. B. Arrieta, N. Díaz-Rodríguez, J. D. Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia, S. GilLopez, D. Molina, R. Benjamins, R. Chatila, F. Herrera. 2020. Explainable artiﬁcial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI, Information Fusion 58, p. 82–115. https://doi.org/10.1016/j.inffus.2019.12.012</p> <p>[6] Peirce Charles. 1934. Collected Papers of Charles Sanders Peirce. vol. 5, Pragmatism and Pragmaticism, edited by Hartshorne C., Weiss P. Cambridge, MA: Harvard University Press.</p> <p>[7] Stefan Timmermans and Iddo Tavory. 2012. Theory Construction in Qualitative Research: From Grounded Theory to Abductive Analysis. Sociological Theory 30, 3, 167–186. https://doi.org/10.1177/0735275112457914</p> <p>[8] Ido Tavory and Stefan Timmermans. 2014. Abductive Analysis: Theorizing Qualitative Research. University of Chicago Press, United States</p>]]></content><author><name></name></author><category term="posts"/><category term="books"/><summary type="html"><![CDATA[A primer on abductive analysis]]></summary></entry><entry><title type="html">AI and the Registry of Power</title><link href="https://timothee-schmude.github.io//blog/2022/ai-atlas/" rel="alternate" type="text/html" title="AI and the Registry of Power"/><published>2022-11-16T10:00:00+00:00</published><updated>2022-11-16T10:00:00+00:00</updated><id>https://timothee-schmude.github.io//blog/2022/ai-atlas</id><content type="html" xml:base="https://timothee-schmude.github.io//blog/2022/ai-atlas/"><![CDATA[<p>I read the <em>Atlas of AI</em>, published in 2021 by Kate Crawford, and it is an astonishing read.</p> <p>We are all quite familiar with the public image of Artificial Intelligence. It is easy to associate the text-to-image generation model <a href="https://timothee-schmude.github.io/blog/2022/dall-e/">Dall-E</a> with the term, or translation software like DeepL, recommender systems used by Netflix or YouTube, and various forms of personal assistants such as Alexa or Siri. Even robots turn out to be a common first thought when talking about AI. But the vital point that Atlas of AI makes is that AI has long eclipsed its usage in mere consumer products and tools. Artificial Intelligence, Kate Crawford argues, is conquering a ”much wider set of political and social structures” and is “ultimately designed to serve existing dominant interests”. In a profound analysis backed by tremendous amounts of research, the book reveals to us that AI is primarily a “registry of power”, and that we should beware.</p> <p>Crawford’s magnificent way of presenting us with this evidence is by telling stories that take us by the hand and say ‘Look, over there, this is the Silver Peak Lithium Mine, where residue from pumping up lithium brine gathers in large, toxic lakes.’, or ‘This is the Armour Beef dressing floor, where workers were the first to take their place in an assembly line, much as they do at Amazon today, where things are spiced up by automated time control.’ Not only are the examples historic, they are graphic, tangible, and they illustrate so well what is missing from our present conception of AI: The dirty, sweaty, greasy foundation on which it is built both materially, and, as it turns out, scientifically.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/ai_atlas_lake-480.webp 480w,/assets/img/posts_images/ai_atlas_lake-800.webp 800w,/assets/img/posts_images/ai_atlas_lake-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/ai_atlas_lake.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Chemetall Foote Lithium Operation in Clayton Valley. Photo by Magnus Manske. </div> <p>My favorite episode in this path of realization is Crawford’s visit to Samuel Morton’s cranial collection, a large amount of skulls gathered and labelled by the craniologist in the 19th century to explore if intelligence and “superiority” could be inferred from the size and structure of the human skull, establishing a deeply flawed and racist take on anatomy. This is the introduction to the book’s section on Classification and it is so rich in history and symbolism that it is nothing short of literary, and thus so suitable for the introduction of a central question: Who is the one holding the power to decide over what ought to be classified, and which classes to use? Of course, Crawford argues, the ones performing these classifications have rarely been the minorities and fringe groups, but the ruling class. It is no large leap of mind to realize that these classifications were not put in place in order to infringe on the ruling class’ control and influence, but to increase and cement them. The achievement of the <em>Atlas of AI</em> is not only to unveil the bizarre building blocks of present-day AI, but also to steer these questions back to the present, connecting them to discussions about bias and surveillance. After taking in the historic panorama that Crawford develops before us, we lastly arrive at the present with a quaint little image of a skull that reminds us of where it all started.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/ai_atlas_skulls-480.webp 480w,/assets/img/posts_images/ai_atlas_skulls-800.webp 800w,/assets/img/posts_images/ai_atlas_skulls-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/ai_atlas_skulls.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Labelled Skulls in Morton cranial collection. Boas, KC "The Curious Cabinet of Dr. Morton" Expedition Magazine 54.3 (2012): n. pag. Expedition Magazine. Penn Museum, 2012 Web. 16 Nov 2022. </div> <p>By leading us from the soil that is exploited for minerals to the integration of large-scale systems in government and military, Crawford carefully shows us how earth, labor, data, classification, state, power, and lastly, space, assume their roles as cogs in the large machinery that is AI. Each of these elements has secrets to tell and by the end we do not assume that anything about AI is without costs, only that they are hidden behind lofty curtains of marketing façade and a general, convenient confusion.</p> <p>I do not want to tell you all the brilliant bits in advance. If you are interested in Artificial Intelligence, working in the field, or reflecting on it, then I wholeheartedly recommend this book and indeed urge you to read it. I had not thought about tracing AI by notions of geography and history in this manner, showing us just how closely connected to political power it is, and so it is not overstating to say that the <em>Atlas of AI</em> opened up my horizon by a considerable margin.</p> <p>Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Kate Crawford, Yale University Press 2021.</p>]]></content><author><name></name></author><category term="posts"/><category term="books"/><summary type="html"><![CDATA[Book review of "AI Atlas"]]></summary></entry><entry><title type="html">Asking DALL·E About The Strongest Avenger; and Other Matters</title><link href="https://timothee-schmude.github.io//blog/2022/dall-e/" rel="alternate" type="text/html" title="Asking DALL·E About The Strongest Avenger; and Other Matters"/><published>2022-06-11T10:00:00+00:00</published><updated>2022-06-11T10:00:00+00:00</updated><id>https://timothee-schmude.github.io//blog/2022/dall-e</id><content type="html" xml:base="https://timothee-schmude.github.io//blog/2022/dall-e/"><![CDATA[<p>You’ve probably heard about <a href="https://openai.com/index/dall-e/">DALL·E</a>, the huge neural network generating images from text, built on top of another huge neural network that is famously well-versed in human language: the Generative Pre-Trained Transformer 3, or GPT-3. So famous, in fact, that OpenAI decided that AI should not be open after all and sealed GPT-3 behind a wall of exclusivity only to be conquered by formal application or by, you know, paying. People that got through this application step tell intriguing tales about all the things that can be done with the model, among them <a href="https://www.aiweirdness.com/gpt-3-tries-pickup-lines/">GPT-3 trying pickup lines</a>, <a href="https://medium.com/swlh/i-wrote-a-book-with-gpt-3-ai-in-24-hours-and-got-it-published-93cf3c96f120">GPT-3 writing books</a>, <a href="https://www.universityofcalifornia.edu/news/will-ai-write-next-great-american-novel">GPT-3 composing poetry</a> and <a href="https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/">GPT-3 doing basically everything else</a>.</p> <p>Setting aside the slightly dystopian implications for the moment, OpenAI in January last year announced yet another machine learning model with impressive capability: DALL·E. This one can generate images from any text that you give it, that is, it takes your words and tries to render a visual interpretation of them. The resulting images are not unlike those from <a href="https://research.google/blog/inceptionism-going-deeper-into-neural-networks/">DeepDream</a> and <a href="https://thispersondoesnotexist.com">thispersondoesnotexist.com</a> (though those are different architectures). Apparently, the most intuitive thing to ask such a machine is to produce an image of a chair in the form of an avocado, which promptly became DALL·E’s emblem.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_avocado_chair-480.webp 480w,/assets/img/posts_images/dall_e_avocado_chair-800.webp 800w,/assets/img/posts_images/dall_e_avocado_chair-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_avocado_chair.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Avocado chair. (Copyright @ OpenAI 2021, or DALL·E, I don't know which) </div> <p>The name DALL·E is doubly punned, referring to our cute little order-obsessed robot WALL-E and the surrealist painter Dalí at the same time, and makes clear the intended crossover of algorithmic structure and artistic vision bordering on the subconscious. In theory this should result in a singular view on the world: a direct transport line between verbal imagination and graphic depiction, heavily influenced by whatever transformation the model performs on the input. And indeed the results appear to live up to this promise. Whether it imagines something realistic, like a pentagonal clock, or something more extraordinary, like a snail harp (images Copyright @ OpenAI 2021).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_green_clock-480.webp 480w,/assets/img/posts_images/dall_e_green_clock-800.webp 800w,/assets/img/posts_images/dall_e_green_clock-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_green_clock.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_snail_harp-480.webp 480w,/assets/img/posts_images/dall_e_snail_harp-800.webp 800w,/assets/img/posts_images/dall_e_snail_harp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_snail_harp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One is well advised not to forget the human part about these image generations. GPT-3, DALL·E’s base model, was trained on numerous texts from the web (the so-called Common Crawl and the whole of Wikipedia) and a lot of books. “Numerous” is understating: they are gigantic amounts. Towering, unimaginable amounts, impossible to grasp – you know the scales. The point is: most of this is human language. Every piece of text has an author, a date of publication, a context, and human views and opinions embedded deeply into it. This is no bad thing! It just means that when we are looking at GPT-3’s output, we are looking at the (mostly intransparent) transformation of a large volume of human language, cast into a specific form, like a distillation of everything that was said in the last 100 years – by certain people, in certain places.</p> <p>On a side note: The process of transforming various inputs by the workings of an opaque machinery into something new actually sounds familiar – humans do it all the time. Of course, this is no clean analogy, there are multiple things we don’t understand about consciousness and the brain. Ironically though, we don’t understand everything about neural networks either. Our not-knowing how is a pretty straight parallel between the two concepts (and something we work on in <a href="[url](https://media.licdn.com/dms/image/v2/D4E12AQEAbSeLC_M0IA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1654948411676?e=1746057600&amp;v=beta&amp;t=FnIkQJfdVHQRyPU8Cy414VnuqxCLeMNCNqSfCNkivDc)">our research project</a>).</p> <p>DALL·E was trained in a similar fashion to GPT-3: by collecting numerous text and image pairs from the internet, since it’s cheap and it’s there. One part comes from Wikipedia, the other part from Flickr. This means we get data from people hanging out on a) Wikipedia, and b) Flickr. Again, nothing to fret about necessarily, just something to keep in mind when looking at the applications of DALL·E. As a reminder, facial recognition systems couldn’t (or might still be unable to) handle images of black women as they were underrepresented in the training data. Thankfully, there are researchers occupying themselves with revealing and dealing with these discriminatory biases. Unluckily, big tech doesn’t always seem to like what they find and prefers to sanction them. Two prominent researches who left (or were made to leave) Google because of this are Margaret Mitchell and Timnit Gebru.</p> <p>Coming back to the practical part of the model, you certainly would like to try your hands at it yourself. Good news! There is a <a href="[url](https://huggingface.co/spaces/dalle-mini/dalle-mini)">mini version of DALL·E over at HuggingFace</a> that let’s you play around with it, it is slow and not as good as the original, but still a lot of fun. You can also build the thing yourself, if you want, they put the <a href="[url](https://github.com/borisdayma/dalle-mini)">code for DALL·E Mini on Github</a>. (HuggingFace really are accommodating in giving access)</p> <p>Alright, let’s see if it delivers! Avocado chairs and pentagon clocks fine and well, but what if we ask for things that can’t be pictured? Like the flavour of the sun?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_orange_sun-480.webp 480w,/assets/img/posts_images/dall_e_orange_sun-800.webp 800w,/assets/img/posts_images/dall_e_orange_sun-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_orange_sun.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We could have figured there would be some orangey vibes in there. Let’s make it more abstract, let’s picture canonical thinking.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_canonical_thinking-480.webp 480w,/assets/img/posts_images/dall_e_canonical_thinking-800.webp 800w,/assets/img/posts_images/dall_e_canonical_thinking-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_canonical_thinking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Whatever it is, the pope appears to have a part in it, and he really makes an effort, too. Not too surprising when you look at the meanings of “canonical”. One could make a sublime joke about the “literature pope” Marcel Reich-Ranicki, as he was called, who published the canon for German literature, but I think that’s going too far.</p> <p>Let’s see what DALL·E does with something even more vague, a certain way of death, that the universe faces, provided some peculiar part of physics will turn out a specific way, the vacuum decay. In short, a bubble with a better energetic state than the present one that forms somewhere in space and makes everything nonexistent very quickly. No alt text provided for this image</p> <p>Yep, that’s not so far off. There are bubbles, certainly, and Wikipedia has an image that virtually looks the same (don’t be confused it actually shows the Cosmic Microwave Background), so let’s settle on that being a truthful depiction of <a href="https://en.wikipedia.org/wiki/False_vacuum">vacuum decay</a>, no one can prove otherwise anyway.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_vacuum_decay-480.webp 480w,/assets/img/posts_images/dall_e_vacuum_decay-800.webp 800w,/assets/img/posts_images/dall_e_vacuum_decay-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_vacuum_decay.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Two to go, ordered by difficulty to answer. First one: Who is the strongest avenger?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_avenger-480.webp 480w,/assets/img/posts_images/dall_e_avenger-800.webp 800w,/assets/img/posts_images/dall_e_avenger-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_avenger.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Kind of all of them, I guess. Though Iron Man plays a central role, but maybe that’s just nostalgia.</p> <p>Last question for the mini-AI-oracle and for our little excursion into strange AI imagination: What does life after death look like?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_images/dall_e_life-480.webp 480w,/assets/img/posts_images/dall_e_life-800.webp 800w,/assets/img/posts_images/dall_e_life-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts_images/dall_e_life.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There is light, dark figures and a black vignette. If you do a google image search you will see that this is rather prototypical. Still, as a distillation of human ideas about life after death, this is not bad at all. As someone who also dove into literature analysis a couple of times, the possibility of interpreting symbolism and composition of these images is very intriguing. Incidentally, I have done something similar in my master’s thesis, but that’s a story for another time.</p> <p>Suppose we had a fully working model of DALL·E, imagine the implications. Every photograph, be it for ads or articles, could be generated, every book cover, every movie poster, theoretically, every frame of a movie as well. This is science fiction, of course, but it slowly transcends into the realm of reality. Indeed, if you can make a profit on it, the thing is shoved into reality in the bat of an eye. And what do you know, there is a new version already! OpenAI presents: <a href="https://openai.com/index/dall-e-2/">DALL·E 2</a>.</p> <p>The important question is, if AI was perfect, and if AI could generate every image possible, would you still look at a human one?</p>]]></content><author><name></name></author><category term="posts"/><category term="genAI"/><summary type="html"><![CDATA[Playing around with image generating AI]]></summary></entry></feed>