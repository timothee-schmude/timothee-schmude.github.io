<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Timothée Schmude </title> <meta name="author" content="Timothée Schmude"> <meta name="description" content="Website of Timothée Schmude. "> <meta name="keywords" content="timothee-schmude, artificial intelligence, explainable ai, contestable ai, university of vienna, ai novices"> <meta property="og:site_name" content="Timothée Schmude"> <meta property="og:type" content="website"> <meta property="og:title" content="Timothée Schmude | publications"> <meta property="og:url" content="https://timothee-schmude.github.io//publications/"> <meta property="og:description" content="Website of Timothée Schmude. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="Website of Timothée Schmude. "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Timothée Schmude"
        },
        "url": "https://timothee-schmude.github.io//publications/",
        "@type": "WebSite",
        "description": "Website of Timothée Schmude.
",
        "headline": "publications",
        
        "name": "Timothée Schmude",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://timothee-schmude.github.io//publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Timothée</span> Schmude </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmude,+T" rel="external nofollow noopener" target="_blank">Preprint</a> </abbr> </div> <div id="schmude2025twomeansendgoal" class="col-sm-8"> <div class="title">"Two Means to an End Goal": Connecting Explainability and Contestability in the Regulation of Public Sector AI</div> <div class="author"> <em>Timothée Schmude</em>, Mireia Yurrita, Kars Alfrink, Thomas Le Goff, and Tiphaine Viard </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.48550/arXiv.2504.18236" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al.%20-%202025%20-%20Two%20Means%20to%20an%20End%20Goal_Connecting%20Explainability%20and%20Contestability%20in%20the%20Regulation%20of%20Public%20Sector%20AI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Explainability and its emerging counterpart contestability have become important normative and design principles for the trustworthy use of AI as they enable users and subjects to understand and challenge AI decisions. However, the regulation of AI systems spans technical, legal, and organizational dimensions, producing a multiplicity in meaning that complicates the implementation of explainability and contestability. Resolving this conceptual ambiguity requires specifying and comparing the meaning of both principles across regulation dimensions, disciplines, and actors. This process, here defined as translation, is essential to provide guidance on the principles’ realization. We present the findings of a semi-structured interview study with 14 interdisciplinary AI regulation experts. We report on the experts’ understanding of the intersection between explainability and contestability in public AI regulation, their advice for a decision subject and a public agency in a welfare allocation AI use case, and their perspectives on the connections and gaps within the research landscape. We provide differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further outline three translation processes in the alignment of top-down and bottom-up regulation, the assignment of responsibility for interpreting regulations, and the establishment of interdisciplinary collaboration. Our contributions include an empirically grounded conceptualization of the intersection between explainability and contestability and recommendations on implementing these principles in public institutions. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://dl.acm.org/doi/10.1145/3706599.3721096" rel="external nofollow noopener" target="_blank">CHI EA 25</a> </abbr> </div> <div id="schmude2025explainandcontest" class="col-sm-8"> <div class="title">Explainability and Contestability for the Responsible Use of Public Sector AI</div> <div class="author"> <em>Timothée Schmude</em> </div> <div class="periodical"> <em>In Proceedings of the Extended Abstracts of CHI’ 25</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706599.3721096" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al.%20-%202025%20-%20Explainability%20and%20Contestability%20for%20the%20Responsible%20Use%20of%20Public%20Sector%20AI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Public institutions have begun to use AI systems in areas that directly impact people’s lives, including labor, law, health, and migration. Explainability ensures that these systems are understandable to the involved stakeholders, while its emerging counterpart contestability enables them to challenge AI decisions. Both principles support the responsible use of AI systems, but their implementation needs to take into account the needs of people without technical background, AI novices. I conduct interviews and workshops to explore how explainable AI can be made suitable for AI novices, how explanations can support their agency by allowing them to contest decisions, and how this intersection is conceptualized. My research aims to inform policy and public institutions on how to implement responsible AI by designing for explainability and contestability. The Remote Doctoral Consortium would allow me to discuss with peers how these principles can be realized and account for human factors in their design.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmude,+T" rel="external nofollow noopener" target="_blank">Preprint</a> </abbr> </div> <div id="schmude2025bettertogether" class="col-sm-8"> <div class="title">Better Together? The Role of Explanations in Supporting Novices in Individual and Collective Deliberations about AI</div> <div class="author"> <em>Timothée Schmude</em>, Laura Koesten, Torsten Möller, and Sebastian Tschiatschek </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.48550/arXiv.2411.11449" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al%20-%202025%20-%20Better%20Together%20-%20The%20Role%20of%20Explanations%20in%20Supporting%20Novices%20in%20Individual%20and%20Collective%20Deliberations%20about%20AI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Deploying AI systems in public institutions can have far-reaching consequences for many people, making it a matter of public interest. Providing opportunities for stakeholders to come together, understand these systems, and debate their merits and harms is thus essential. Explainable AI often focuses on individuals, but deliberation benefits from group settings, which are underexplored. To address this gap, we present findings from an interview study with 8 focus groups and 12 individuals. Our findings provide insight into how explanations support AI novices in deliberating alone and in groups. Participants used modular explanations with four information categories to solve tasks and decide about an AI system’s deployment. We found that the explanations supported groups in creating shared understanding and in finding arguments for and against the system’s deployment. In comparison, individual participants engaged with explanations in more depth and performed better in the study tasks, but missed an exchange with others. Based on our findings, we provide suggestions on how explanations should be designed to work in group settings and describe their potential use in real-world contexts. With this, our contributions inform XAI research that aims to enable AI novices to understand and deliberate AI systems in the public sector.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://doi.org/10.1016/j.ijhcs.2024.103380" rel="external nofollow noopener" target="_blank">IJHCS</a> </abbr> </div> <div id="schmude2024_ai_novices" class="col-sm-8"> <div class="title">Information that matters: Exploring information needs of people affected by algorithmic decisions</div> <div class="author"> <em>Timothée Schmude</em>, Laura Koesten, Torsten Möller, and Sebastian Tschiatschek </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.ijhcs.2024.103380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al.%20-%202024%20-%20Information%20That%20Matters_Exploring%20Information%20Needs%20of%20People%20Affected%20by%20Algorithmic%20Decisions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system’s decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system’s deployment, while those who perceived low risks rather asked about the system’s operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmude,+T" rel="external nofollow noopener" target="_blank">Preprint</a> </abbr> </div> <div id="tschiatschek2024" class="col-sm-8"> <div class="title">Challenging the Human-in-the-loop in Algorithmic Decision-making</div> <div class="author"> Sebastian Tschiatschek, Eugenia Stamboliev, <em>Timothée Schmude</em>, Mark Coeckelbergh, and Laura Koesten </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.48550/arXiv.2405.10706" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Tschiatschek%20et%20al.%20-%202024%20-%20Challenging%20the%20Human-in-the-Loop%20in%20Algorithmic%20Decision-Making.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We discuss the role of humans in algorithmic decision-making (ADM) for socially relevant problems from a technical and philosophical perspective. In particular, we illustrate tensions arising from diverse expectations, values, and constraints by and on the humans involved. To this end, we assume that a strategic decision-maker (SDM) introduces ADM to optimize strategic and societal goals while the algorithms’ recommended actions are overseen by a practical decision-maker (PDM) - a specific human-in-the-loop - who makes the final decisions. While the PDM is typically assumed to be a corrective, it can counteract the realization of the SDM’s desired goals and societal values not least because of a misalignment of these values and unmet information needs of the PDM. This has significant implications for the distribution of power between the stakeholders in ADM, their constraints, and information needs. In particular, we emphasize the overseeing PDM’s role as a potential political and ethical decision maker, who acts expected to balance strategic, value-driven objectives and on-the-ground individual decisions and constraints. We demonstrate empirically, on a machine learning benchmark dataset, the significant impact an overseeing PDM’s decisions can have even if the PDM is constrained to performing only a limited amount of actions differing from the algorithms’ recommendations. To ensure that the SDM’s intended values are realized, the PDM needs to be provided with appropriate information conveyed through tailored explanations and its role must be characterized clearly. Our findings emphasize the need for an in-depth discussion of …</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://biblioscout.net/article/10.33196/juridikum202402024401" rel="external nofollow noopener" target="_blank">juridikum</a> </abbr> </div> <div id="paar2024spotlight" class="col-sm-8"> <div class="title">Spotlight Erklärbare KI: Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive</div> <div class="author"> Elisabeth Paar, <em>Timothée Schmude</em>, and Cansu Cinar </div> <div class="periodical"> <em>Juridikum. Zeitschrift für Kritik, Recht, Gesellschaft</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.33196/juridikum202402024401" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Paar,%20Schmude,%20Cinar%20-%202024%20-%20Spotlight%20Erkl%C3%A4rbare%20KI_Eine%20Besprechung%20ausgew%C3%A4hlter%20Use%20Cases%20aus%20rechtlicher%20und%20technologischer%20Perspektive.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Spotlight Erklärbare KI: Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive - University of Vienna Skip to main navigation Skip to search Skip to main content University of Vienna Home University of Vienna Logo u:cris Info Deutsch English Home Persons Units and bodies Projects Publications Activities Prizes Press/Media Search by expertise, name or affiliation Spotlight Erklärbare KI: Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive Elisabeth Paar, Timothée Schmude, Cansu Cinar Department of Legal and Constitutional History Research Network Data Science Publications: Contribution to journal › Article › Peer Reviewed Overview Original language German Pages (from-to) 244 Number of pages 253 Journal Juridikum. Zeitschrift für Kritik, Recht, Gesellschaft Publication status Published - 2024 Austrian Fields of Science 2012 …</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594054" rel="external nofollow noopener" target="_blank">FAccT 23</a> </abbr> </div> <div id="schmude2023" class="col-sm-8"> <div class="title">On the Impact of Explanations on Understanding of Algorithmic Decision-Making</div> <div class="author"> <em>Timothée Schmude</em>, Laura Koesten, Torsten Möller, and Sebastian Tschiatschek </div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>, Chicago, IL, USA, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3593013.3594054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al.%20-%202023%20-%20On%20the%20Impact%20of%20Explanations%20on%20Understanding%20of%20Algorithmic%20Decision-Making.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by "high-risk" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a "high-risk" ADM system to participants and analyse their responses both inductively and deductively, using the "six facets of understanding" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the "six facets" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the "dialogue" modality as a valid explanation approach to increase participant engagement and interaction with the "explainer", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the "six facets" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmude,+T" rel="external nofollow noopener" target="_blank">Preprint</a> </abbr> </div> <div id="schmude2023interdisciplinary_frameworks" class="col-sm-8"> <div class="title">Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making</div> <div class="author"> <em>Timothée Schmude</em>, Laura Koesten, Torsten Möller, and Sebastian Tschiatschek </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.48550/arXiv.2305.16700" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20et%20al.%20-%202023%20-%20Applying%20Interdisciplinary%20Frameworks%20to%20Understand%20Algorithmic%20Decision-Making.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We argue that explanations for "algorithmic decision-making" (ADM) systems can profit by adopting practices that are already used in the learning sciences. We shortly introduce the importance of explaining ADM systems, give a brief overview of approaches drawing from other disciplines to improve explanations, and present the results of our qualitative task-based study incorporating the "six facets of understanding" framework. We close with questions guiding the discussion of how future studies can leverage an interdisciplinary approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://dl.acm.org/doi/10.1145/3604915.3608754" rel="external nofollow noopener" target="_blank">RecSys 23</a> </abbr> </div> <div id="inel2023" class="col-sm-8"> <div class="title">QUARE: 2nd Workshop on Measuring the Quality of Explanations in Recommender Systems</div> <div class="author"> Oana Inel, Nicolas Mattis, Milda Norkute, Alessandro Piscopo, <em>Timothée Schmude</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sanne Vrijenhoek, Krisztian Balog' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 17th ACM Conference on Recommender Systems</em>, Singapore, Singapore, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3604915.3608754" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Inel%20et%20al.%20-%202023%20-%20QUARE%202nd%20Workshop%20on%20Measuring%20the%20Quality%20of%20Explanations%20in%20Recommender%20Systems.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>QUARE — measuring the QUality of explAnations in REcommender systems — is the second workshop which focuses on evaluation methodologies for explanations in recommender systems. We bring together researchers and practitioners from academia and industry to facilitate discussions about the main issues and best practices in the respective areas, identify possible synergies, and outline priorities regarding future research directions. Additionally, we want to stimulate reflections around methods to systematically and holistically assess explanation approaches, impact, and goals, at the interplay between organisational and human values. To that end, this workshop aims to co-create a research agenda for evaluating the quality of explanations for recommender systems.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://zenodo.org/records/7319832" rel="external nofollow noopener" target="_blank">AI Campus</a> </abbr> </div> <div id="schmude2022" class="col-sm-8"> <div class="title">Program or be Programmed: Lehre Künstlicher Intelligenz in den Digital Humanities</div> <div class="author"> <em>Timothée Schmude</em>, and Claes Neuefeind </div> <div class="periodical"> <em>Hochschullehre zu Künstlicher Intelligenz</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.5281/zenodo.7319831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="/assets/pdf/Schmude%20und%20Neuefeind%20-%202022%20-%20Program%20or%20be%20Programmed_Lehre%20K%C3%BCnstlicher%20Intelligenz%20in%20den%20Digital%20Humanities.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In unserem Beitrag stellen wir einen Ansatz zur Vermittlung von KI-bezogenen Themen in den Digital Humanities (DH) vor. Das Konzept besteht aus einer Parallelführung von theoretischem Seminar und praktischer Übung, die wir im Studienjahr 2021/2022 an der Universität zu Köln realisiert haben. Leitgedanke des Beitrags ist dabei, dass die DH als interdisziplinäres Forschungsfeld an der Schnittstelle zwischen Geisteswissenschaften und digitalen Technologien ein besonders geeignetes Umfeld bieten, um Studierende für die vielseitigen Anforderungen im Bereich der KI auszubilden.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://publica-rest.fraunhofer.de/server/api/core/bitstreams/fe0ff71e-dd09-4e2e-bb59-f349deb54887/content" rel="external nofollow noopener" target="_blank">LWDA 2020</a> </abbr> </div> <div id="kirsch2020using" class="col-sm-8"> <div class="title">Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain.</div> <div class="author"> Birgit Kirsch, Sven Giesselbach, <em>Timothée Schmude</em>, Malte Völkening, Frauke Rostalski, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Stefan Rüping' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In LWDA 2020</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://publica-rest.fraunhofer.de/server/api/core/bitstreams/fe0ff71e-dd09-4e2e-bb59-f349deb54887/content" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Kirsch%20et%20al.%20-%202021%20-%20Using%20Probabilistic%20Soft%20Logic%20to%20Improve%20Information%20Extraction%20in%20the%20Legal%20Domain.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Extracting information from court process documents to populate a knowledge base produces data valuable to legal faculties, publishers and law firms. A challenge lies in the fact that the relevant information is interdependent and structured by numerous semantic constraints of the legal domain. Ignoring these dependencies leads to inferior solutions. Hence, the objective of this paper is to demonstrate how the extraction pipeline can be improved by the use of probabilistic soft logic rules that reflect both legal and linguistic knowledge. We propose a probabilistic rule model for the overall extraction pipeline, which enables to both map dependencies between local extraction models and to integrate additional domain knowledge in the form of logical constraints. We evaluate the performance of the model on a German court sentences corpus.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Timothée Schmude. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>