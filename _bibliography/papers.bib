---
---

@inproceedings{schmude2023,
author = {Schmude, Timoth\'{e}e and Koesten, Laura and M\"{o}ller, Torsten and Tschiatschek, Sebastian},
title = {On the Impact of Explanations on Understanding of Algorithmic Decision-Making},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594054},
doi = {10.1145/3593013.3594054},
abstract = {Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by "high-risk" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a "high-risk" ADM system to participants and analyse their responses both inductively and deductively, using the "six facets of understanding" framework by Wiggins \& McTighe [63]. Our findings indicate that the "six facets" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the "dialogue" modality as a valid explanation approach to increase participant engagement and interaction with the "explainer", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the "six facets" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {959–970},
numpages = {12},
keywords = {XAI, algorithmic decision-making, algorithmic fairness, learning Sciences, qualitative methods},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{schmude2024_xainqb,
title = {Information that matters: Exploring information needs of people affected by algorithmic decisions},
journal = {International Journal of Human-Computer Studies},
volume = {193},
pages = {103380},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2024.103380},
url = {https://www.sciencedirect.com/science/article/pii/S1071581924001630},
author = {Timothée Schmude and Laura Koesten and Torsten Möller and Sebastian Tschiatschek},
keywords = {Explainable AI, Understanding, Information needs, Affected stakeholders, Question-driven explanations, Qualitative methods},
abstract = {Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system’s decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system’s deployment, while those who perceived low risks rather asked about the system’s operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.}
}

@article{schmude2024_deliberative,
  title={Deliberative XAI: How Explanations Impact Understanding and Decision-Making of AI Novices in Collective and Individual Settings},
  author={Schmude, Timoth{\'e}e and Koesten, Laura and M{\"o}ller, Torsten and Tschiatschek, Sebastian},
  journal={arXiv preprint arXiv:2411.11449},
  year={2024}
}

@article{tschiatschek2024challenging,
  title={Challenging the Human-in-the-loop in Algorithmic Decision-making},
  author={Tschiatschek, Sebastian and Stamboliev, Eugenia and Schmude, Timoth{\'e}e and Coeckelbergh, Mark and Koesten, Laura},
  journal={arXiv preprint arXiv:2405.10706},
  year={2024}
}

@article{paar2024spotlight,
  title={Spotlight Erkl{\"a}rbare KI: Eine Besprechung ausgew{\"a}hlter Use Cases aus rechtlicher und technologischer Perspektive},
  author={Paar, Elisabeth and Schmude, Timoth{\'e}e and Cinar, Cansu},
  journal={Juridikum. Zeitschrift f{\"u}r Kritik, Recht, Gesellschaft},
  pages={244},
  year={2024},
  publisher={Verl. {\"O}sterreich}
}

@article{schmude2023applying,
  title={Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making},
  author={Schmude, Timoth{\'e}e and Koesten, Laura and M{\"o}ller, Torsten and Tschiatschek, Sebastian},
  journal={arXiv preprint arXiv:2305.16700},
  year={2023}
}

@inproceedings{kirsch2020using,
  title={Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain.},
  author={Kirsch, Birgit and Giesselbach, Sven and Schmude, Timoth{\'e}e and V{\"o}lkening, Malte and Rostalski, Frauke and R{\"u}ping, Stefan},
  booktitle={LWDA},
  pages={76--87},
  year={2020}
}

@article{schmudeprogram,
  title={Program or be Programmed: Lehre K{\"u}nstlicher Intelligenz in den Digital Humanities},
  author={Schmude, Timoth{\'e}e and Neuefeind, Claes},
  journal={Hochschullehre zu K{\"u}nstlicher Intelligenz},
  pages={32}
}

