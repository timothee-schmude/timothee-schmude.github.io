---
---

@misc{schmude2024_deliberative_xai,
  abbr = {arXiv XAI},
  title={Deliberative XAI: How Explanations Impact Understanding and Decision-Making of AI Novices in Collective and Individual Settings}, 
  author={Timothée Schmude and Laura Koesten and Torsten Möller and Sebastian Tschiatschek},
  year={2024},
  eprint={2411.11449},
  archivePrefix={arXiv},
  primaryClass={cs.HC},
  doi={https://doi.org/10.48550/arXiv.2411.11449},
  pdf={Schmude et al. - 2024 - Deliberative XAI How Explanations Impact Understanding and Decision-Making of AI Novices in Collective and Individual Settings.pdf},
  url={https://arxiv.org/abs/2411.11449}, 
  abstract={XAI research often focuses on settings where people learn about and assess algorithmic systems individually. However, as more public AI systems are deployed, it becomes essential for XAI to facilitate collective understanding and deliberation. We conducted a task-based interview study involving 8 focus groups and 12 individual interviews to explore how explanations can support AI novices in understanding and forming opinions about AI systems. Participants received a collection of explanations organized into four information categories to solve tasks and decide about a system's deployment. These explanations improved or calibrated participants' self-reported understanding and decision confidence and facilitated group discussions. Participants valued both technical and contextual information and the self-directed and modular explanation structure. Our contributions include an explanation approach that facilitates both individual and collaborative interaction and explanation design recommendations, including active and controllable exploration, different levels of information detail and breadth, and adaptations to the needs of decision subjects.},
  selected = {true}
}

@article{schmude2024_ai_novices,
  abbr = {IJHCS},
  title = {Information that matters: Exploring information needs of people affected by algorithmic decisions},
  journal = {International Journal of Human-Computer Studies},
  volume = {193},
  pages = {103380},
  year = {2024},
  issn = {1071-5819},
  pdf={Schmude et al. - 2024 - Information That Matters_Exploring Information Needs of People Affected by Algorithmic Decisions.pdf},
  doi = {https://doi.org/10.1016/j.ijhcs.2024.103380},
  url = {https://www.sciencedirect.com/science/article/pii/S1071581924001630},
  author = {Timothée Schmude and Laura Koesten and Torsten Möller and Sebastian Tschiatschek},
  keywords = {Explainable AI, Understanding, Information needs, Affected stakeholders, Question-driven explanations, Qualitative methods},
  preview={XAI Novice Question Bank.png},
  abstract={Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system’s decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system’s deployment, while those who perceived low risks rather asked about the system’s operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.},
  selected = {true}
}

@inproceedings{schmude2023,
  abbr = {FAccT 23},
  author = {Schmude, Timoth\'{e}e and Koesten, Laura and M\"{o}ller, Torsten and Tschiatschek, Sebastian},
  title = {On the Impact of Explanations on Understanding of Algorithmic Decision-Making},
  year = {2023},
  isbn = {9798400701924},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3593013.3594054},
  doi = {10.1145/3593013.3594054},
  pdf={Schmude et al. - 2023 - On the Impact of Explanations on Understanding of Algorithmic Decision-Making.pdf},
  abstract = {Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by "high-risk" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a "high-risk" ADM system to participants and analyse their responses both inductively and deductively, using the "six facets of understanding" framework by Wiggins \& McTighe [63]. Our findings indicate that the "six facets" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the "dialogue" modality as a valid explanation approach to increase participant engagement and interaction with the "explainer", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the "six facets" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  pages = {959–970},
  numpages = {12},
  keywords = {XAI, algorithmic decision-making, algorithmic fairness, learning Sciences, qualitative methods},
  location = {Chicago, IL, USA},
  series = {FAccT '23},
  selected = {true}
}

@misc{tschiatschek2024,
  abbr = {arXiv HIL},
  title={Challenging the Human-in-the-loop in Algorithmic Decision-making}, 
  author={Sebastian Tschiatschek and Eugenia Stamboliev and Timothée Schmude and Mark Coeckelbergh and Laura Koesten},
  year={2024},
  eprint={2405.10706},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  doi={https://doi.org/10.48550/arXiv.2405.10706},
  pdf={Tschiatschek et al. - 2024 - Challenging the Human-in-the-Loop in Algorithmic Decision-Making.pdf},
  abstract={We discuss the role of humans in algorithmic decision-making (ADM) for socially relevant problems from a technical and philosophical perspective. In particular, we illustrate tensions arising from diverse expectations, values, and constraints by and on the humans involved. To this end, we assume that a strategic decision-maker (SDM) introduces ADM to optimize strategic and societal goals while the algorithms' recommended actions are overseen by a practical decision-maker (PDM) - a specific human-in-the-loop - who makes the final decisions. While the PDM is typically assumed to be a corrective, it can counteract the realization of the SDM's desired goals and societal values not least because of a misalignment of these values and unmet information needs of the PDM. This has significant implications for the distribution of power between the stakeholders in ADM, their constraints, and information needs. In particular, we emphasize the overseeing PDM's role as a potential political and ethical decision maker, who acts expected to balance strategic, value-driven objectives and on-the-ground individual decisions and constraints. We demonstrate empirically, on a machine learning benchmark dataset, the significant impact an overseeing PDM's decisions can have even if the PDM is constrained to performing only a limited amount of actions differing from the algorithms' recommendations. To ensure that the SDM's intended values are realized, the PDM needs to be provided with appropriate information conveyed through tailored explanations and its role must be characterized clearly. Our findings emphasize the need for an in-depth discussion of …},
  url={https://arxiv.org/abs/2405.10706}, 
}

@article{paar2024spotlight,
  abbr = {juridikum},
  title={Spotlight Erkl{\"a}rbare KI: Eine Besprechung ausgew{\"a}hlter Use Cases aus rechtlicher und technologischer Perspektive},
  author={Paar, Elisabeth and Schmude, Timoth{\'e}e and Cinar, Cansu},
  journal={Juridikum. Zeitschrift f{\"u}r Kritik, Recht, Gesellschaft},
  pages={244--253},
  year={2024},
  doi={https://doi.org/10.33196/juridikum202402024401},
  pdf={Paar, Schmude, Cinar - 2024 - Spotlight Erklärbare KI_Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive.pdf},
  abstract={Spotlight Erklärbare KI: Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive - University of Vienna Skip to main navigation Skip to search Skip to main content University of Vienna Home University of Vienna Logo u:cris Info Deutsch English Home Persons Units and bodies Projects Publications Activities Prizes Press/Media Search by expertise, name or affiliation Spotlight Erklärbare KI: Eine Besprechung ausgewählter Use Cases aus rechtlicher und technologischer Perspektive Elisabeth Paar, Timothée Schmude, Cansu Cinar Department of Legal and Constitutional History Research Network Data Science Publications: Contribution to journal › Article › Peer Reviewed Overview Original language German Pages (from-to) 244 Number of pages 253 Journal Juridikum. Zeitschrift für Kritik, Recht, Gesellschaft Publication status Published - 2024 Austrian Fields of Science 2012 …},
  publisher={Verl. {\"O}sterreich}
}

@misc{schmude2023interdisciplinary_frameworks,
  abbr = {arXiv Frameworks},
  title={Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making}, 
  author={Timothée Schmude and Laura Koesten and Torsten Möller and Sebastian Tschiatschek},
  year={2023},
  eprint={2305.16700},
  archivePrefix={arXiv},
  primaryClass={cs.HC},
  doi={https://doi.org/10.48550/arXiv.2305.16700},
  pdf={Schmude et al. - 2023 - Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making.pdf},
  abstract={We argue that explanations for "algorithmic decision-making" (ADM) systems can profit by adopting practices that are already used in the learning sciences. We shortly introduce the importance of explaining ADM systems, give a brief overview of approaches drawing from other disciplines to improve explanations, and present the results of our qualitative task-based study incorporating the "six facets of understanding" framework. We close with questions guiding the discussion of how future studies can leverage an interdisciplinary approach.},
  url={https://arxiv.org/abs/2305.16700}, 
}

@inproceedings{inel2023,
  abbr = {RecSys 23},
  author = {Inel, Oana and Mattis, Nicolas and Norkute, Milda and Piscopo, Alessandro and Schmude, Timoth\'{e}e and Vrijenhoek, Sanne and Balog, Krisztian},
  title = {QUARE: 2nd Workshop on Measuring the Quality of Explanations in Recommender Systems},
  year = {2023},
  isbn = {9798400702419},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3604915.3608754},
  doi = {10.1145/3604915.3608754},
  booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
  pages = {1241–1243},
  numpages = {3},
  keywords = {Recommender systems, explanation evaluation, explanation goals, explanation quality},
  pdf={Inel et al. - 2023 - QUARE 2nd Workshop on Measuring the Quality of Explanations in Recommender Systems.pdf},
  location = {Singapore, Singapore},
  abstract={QUARE — measuring the QUality of explAnations in REcommender systems — is the second workshop which focuses on evaluation methodologies for explanations in recommender systems. We bring together researchers and practitioners from academia and industry to facilitate discussions about the main issues and best practices in the respective areas, identify possible synergies, and outline priorities regarding future research directions. Additionally, we want to stimulate reflections around methods to systematically and holistically assess explanation approaches, impact, and goals, at the interplay between organisational and human values. To that end, this workshop aims to co-create a research agenda for evaluating the quality of explanations for recommender systems.},
  series = {RecSys '23}
}

@article{schmude2022,
  abbr = {AI Campus},
  title={Program or be Programmed: Lehre K{\"u}nstlicher Intelligenz in den Digital Humanities},
  author={Schmude, Timoth{\'e}e and Neuefeind, Claes},
  journal={Hochschullehre zu K{\"u}nstlicher Intelligenz},
  doi={https://doi.org/10.5281/zenodo.7319831},
  url={https://www.researchgate.net/profile/Dana-Kristin-Mah/publication/366194166_Anwendungsorientierte_Hochschullehre_zu_Kunstlicher_Intelligenz_Impulse_aus_dem_Fellowship-Programm_zur_Integration_von_KI-Campus-Lernangeboten/links/63971ed411e9f00cda3dc49b/Anwendungsorientierte-Hochschullehre-zu-Kuenstlicher-Intelligenz-Impulse-aus-dem-Fellowship-Programm-zur-Integration-von-KI-Campus-Lernangeboten.pdf#page=39},
  pages={32--43},
  pdf={Schmude und Neuefeind - 2022 - Program or be Programmed_Lehre Künstlicher Intelligenz in den Digital Humanities.pdf},
  abstract={In unserem Beitrag stellen wir einen Ansatz zur Vermittlung von KI-bezogenen Themen in den Digital Humanities (DH) vor. Das Konzept besteht aus einer Parallelführung von theoretischem Seminar und praktischer Übung, die wir im Studienjahr 2021/2022 an der Universität zu Köln realisiert haben. Leitgedanke des Beitrags ist dabei, dass die DH als interdisziplinäres Forschungsfeld an der Schnittstelle zwischen Geisteswissenschaften und digitalen Technologien ein besonders geeignetes Umfeld bieten, um Studierende für die vielseitigen Anforderungen im Bereich der KI auszubilden.},
  year={2022}
}

@inproceedings{kirsch2020using,
  abbr = {Fraunhofer},
  title={Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain.},
  author={Kirsch, Birgit and Giesselbach, Sven and Schmude, Timoth{\'e}e and V{\"o}lkening, Malte and Rostalski, Frauke and R{\"u}ping, Stefan},
  publisher = {LWDA},
  html={https://publica-rest.fraunhofer.de/server/api/core/bitstreams/fe0ff71e-dd09-4e2e-bb59-f349deb54887/content},
  pdf={Kirsch et al. - 2021 - Using Probabilistic Soft Logic to Improve Information Extraction in the Legal Domain.pdf},
  year={2020},
  abstract={Extracting information from court process documents to populate a knowledge base produces data valuable to legal faculties, publishers and law firms. A challenge lies in the fact that the relevant information is interdependent and structured by numerous semantic constraints of the legal domain. Ignoring these dependencies leads to inferior solutions. Hence, the objective of this paper is to demonstrate how the extraction pipeline can be improved by the use of probabilistic soft logic rules that reflect both legal and linguistic knowledge. We propose a probabilistic rule model for the overall extraction pipeline, which enables to both map dependencies between local extraction models and to integrate additional domain knowledge in the form of logical constraints. We evaluate the performance of the model on a German court sentences corpus.},
}


